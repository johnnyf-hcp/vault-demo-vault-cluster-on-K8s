{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# Vault Setup in Kubernetes (EKS)\n",
    "\n",
    "This demo is to show how a Vault cluster can be configured on Kubernetes.  We will also show how Auto Unseal can be configured using the Vault Transit Engine.\n",
    "\n",
    "<img src=\"images/vault-demo-vault-cluster-on-K8s.png\">\n",
    "\n",
    "## Summary of solution\n",
    "\n",
    "This setup is tested on MacOS and is meant to simulate a distributed setup.  In this demo, we will be going through the following steps:\n",
    "- Setup a kind K8s cluster (https://kind.sigs.k8s.io/)\n",
    "- Install and Configure a 3 node Vault cluster using the Vault Helm Chart\n",
    "- Expose the Vault nodes using a NodePort\n",
    "- Demonstrate how automated snapshots configuration and also how manual snapshots are used\n",
    "- Test out the High Availability of the Vault Cluster\n",
    "- Setup Transit Auto-Unseal to simplify Vault server restarts\n",
    "- Upgrading a Vault cluster using integrated storage autopilot\n",
    "\n",
    "## Requirements to Run This Demo\n",
    "You will need Visual Studio Code to be installed with the Jupyter plugin.  To run this notebook in VS Code, chose the Jupyter kernel and then Bash.\n",
    "- To run the current cell, use Ctrl + Enter.\n",
    "- To run the current cell and advance to the next, use Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Pre-requisites (One-time)\n",
    "\n",
    "Assumes you have docker installed and brew installed\n",
    "\n",
    "- https://docs.docker.com/desktop/install/mac-install/\n",
    "- https://brew.sh/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install kind\n",
    "brew install kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install Kubectl CLI\n",
    "brew install kubernetes-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install Helm CLI.  This is used to install the VSO helm chart.\n",
    "brew install helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install K9s.  This is a nice console GUI for K8s.  https://k9scli.io/\n",
    "brew install K9s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup K8s cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Start a kind cluster 3 nodes for the Vault cluster and 1 node for the Transit Auto-Unseal\n",
    "# We will be setting up 6 worker nodes as we will be showing the demo for autopilot upgrade later on.\n",
    "# Note that the Vault helm chart default affinity settings spreads a Vault setup across different host nodes\n",
    "# We will be doing a NodePort on port 30000 so kind needs to configure the extraPortMappings to expose port 30000 to the host\n",
    "kind create cluster --name vault --image kindest/node:v1.28.0 --config - <<EOF\n",
    "kind: Cluster\n",
    "apiVersion: kind.x-k8s.io/v1alpha4\n",
    "nodes:\n",
    "- role: control-plane\n",
    "  extraPortMappings:\n",
    "  - containerPort: 30000\n",
    "    hostPort: 30000\n",
    "    listenAddress: \"0.0.0.0\" # Optional, defaults to \"0.0.0.0\"\n",
    "    protocol: tcp # Optional, defaults to tcp\n",
    "- role: worker\n",
    "- role: worker\n",
    "- role: worker\n",
    "- role: worker\n",
    "- role: worker\n",
    "- role: worker\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Verify kind containers are running\n",
    "docker ps\n",
    "echo\n",
    "# Show that we have 6 nodes in our K8s cluster\n",
    "kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new K8s namespace for this demo\n",
    "## Specify the K8s namespace for the Vault setup\n",
    "export KUBENAMESPACE=vault-ns\n",
    "\n",
    "## Delete namespace if it exists\n",
    "#kubectl delete ns $KUBENAMESPACE\n",
    "\n",
    "echo \"Creating K8s namespace: $KUBENAMESPACE\"\n",
    "kubectl create ns $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Setup Vault Enterprise License in a K8s secret.  Update the path to your license file.\n",
    "export VAULT_LICENCE=$(cat ../../vault-enterprise/vault_local/data/vault.hclic)\n",
    "#kubectl delete secret vault-ent-license -n $KUBENAMESPACE\n",
    "kubectl create secret generic vault-ent-license --from-literal=\"license=${VAULT_LICENCE}\" -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# We will be exposing the vault nodes using a NodePort on port 30000\n",
    "# vault-active: \"true\" is commented out.  If included, it will only route to the leader node\n",
    "kubectl apply -n $KUBENAMESPACE -f - <<EOF\n",
    "kind: Service\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: port-vault-svc\n",
    "spec:\n",
    "  type: NodePort \n",
    "  selector:\n",
    "    app.kubernetes.io/name: \"vault\"\n",
    "    app.kubernetes.io/instance: \"vault\"\n",
    "    component: server\n",
    "    #vault-active: \"true\"\n",
    "  ports:\n",
    "    - nodePort: 30000\n",
    "      port: 8200\n",
    "      targetPort: 8200\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configure my host to connect to the NodePort for Vault\n",
    "export VAULT_ADDR=http://localhost:30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Add the HashiCorp repo (Only required for the first time)\n",
    "helm repo add hashicorp https://helm.releases.hashicorp.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Optional.  Update the repo (Only required when new versions are released)\n",
    "helm repo update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Optional.  This allows you to view the helm charts for vault\n",
    "helm search repo hashicorp/vault -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a new 3 node Vault Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install a 3 node Vault cluster using the Vault helm chart.  \n",
    "# This will configure the raft database on PersistentVolumes and also configure raft auto join between the 3 Vault pods.\n",
    "# For demo purposes, we will be using HTTP.\n",
    "# See https://developer.hashicorp.com/vault/docs/platform/k8s/helm/configuration for options\n",
    "helm install vault hashicorp/vault --version 0.27.0 -n $KUBENAMESPACE -f - <<EOF\n",
    "injector:\n",
    "  enabled: false\n",
    "server:\n",
    "  image:\n",
    "    repository: hashicorp/vault-enterprise\n",
    "    tag: latest\n",
    "  enterpriseLicense:\n",
    "    secretName: vault-ent-license\n",
    "  logLevel: trace\n",
    "  auditStorage:\n",
    "    enabled: true\n",
    "  ha:\n",
    "    enabled: true\n",
    "    replicas: 3\n",
    "    raft:\n",
    "      enabled: true\n",
    "      setNodeId: true\n",
    "      config: |\n",
    "        disable_mlock = true\n",
    "        ui = true\n",
    "        listener \"tcp\" {\n",
    "          tls_disable = 1\n",
    "          address = \"[::]:8200\"\n",
    "          cluster_address = \"[::]:8201\"\n",
    "        }\n",
    "        storage \"raft\" {\n",
    "          # PVC Volume to keep Vault data\n",
    "          path = \"/vault/data\"\n",
    "          # For auto-join to the raft cluster\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-0.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-1.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-2.vault-internal:8200\"\n",
    "          } \n",
    "        }\n",
    "EOF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View installed charts\n",
    "helm list -A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View Vault pods in Vault namespace\n",
    "#kubectl get pods -n $KUBENAMESPACE -o wide\n",
    "\n",
    "# Show resources in Vault namespace\n",
    "kubectl -n $KUBENAMESPACE get all\n",
    "\n",
    "# Make sure all Vault pods are in Running status before continuing\n",
    "\n",
    "# Note:\n",
    "# The containers should start within less than a minute.  If the containers get stuck in ContainerCreating for very long without any errors.\n",
    "# There could be throttling issues on the DockerHub side.  You might want to kill and restart the kind cluster and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# On first time setup, verify that all Vault nodes are sealed and not initialized.  Initialized = falase & Sealed = true\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault status\n",
    "echo\n",
    "kubectl exec -ti vault-1 -n $KUBENAMESPACE -- vault status\n",
    "echo\n",
    "kubectl exec -ti vault-2 -n $KUBENAMESPACE -- vault status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize vault-0 pod.  For demo purposes, we will just be generating 1 unseal key.\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault operator init -format=json -key-shares=1 -key-threshold=1 > init.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show the init.json\n",
    "cat init.json | jq\n",
    "\n",
    "# Store the Unseal Key and Root Token for use later\n",
    "export UNSEAL_KEY=$(jq -r '.unseal_keys_b64[]' init.json)\n",
    "export VAULT_TOKEN=$(jq -r '.root_token' init.json)\n",
    "echo\n",
    "echo \"Vault Unseal Key: $UNSEAL_KEY\"\n",
    "echo \"Vault Root Token: $VAULT_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Unseal vault-0 pod.  You should see Sealed = false.  Re-run the command if Sealed is true.\n",
    "echo \"Vault Unseal Key: $UNSEAL_KEY\"\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault operator unseal $UNSEAL_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Note: This step is only required if the retry_join setting is not in the Vault config.\n",
    "# We are skipping this step but note that you can do manual joining if you don't specify retry_join in the Vault config.\n",
    "# Join vault-1 pod to the cluster.\n",
    "#kubectl exec -ti vault-1 -n $KUBENAMESPACE -- vault operator raft join http://vault-0.vault-internal:8200\n",
    "# Join vault-2 pod to the cluster.\n",
    "#kubectl exec -ti vault-2 -n $KUBENAMESPACE -- vault operator raft join http://vault-0.vault-internal:8200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Unseal vault-1 pod.  You should see Sealed = false.  Re-run the command if Sealed is true.\n",
    "echo \"Vault Unseal Key: $UNSEAL_KEY\"\n",
    "kubectl exec -ti vault-1 -n $KUBENAMESPACE -- vault operator unseal $UNSEAL_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Unseal vault-2 pod.  You should see Sealed = false.  Re-run the command if Sealed is true.\n",
    "echo \"Vault Unseal Key: $UNSEAL_KEY\"\n",
    "kubectl exec -ti vault-2 -n $KUBENAMESPACE -- vault operator unseal $UNSEAL_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Verify that I can access the vault cluster from the node port\n",
    "vault secrets list\n",
    "echo\n",
    "# Test logging in as root on vault-0 and verify that you can also access vault from the pod\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault login $VAULT_TOKEN\n",
    "echo\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault secrets list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vault Backup and Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configure automated snapshots every 24 hours.  The snapshots are stored locally in a directory named \n",
    "# \"vault\" and retain 7 snapshots before one can be deleted to make room for the next snapshot. \n",
    "# The local disk space available to store the snapshot is 1GB or 1073741824 bytes. This means that raft-backup retains up to 7 snapshots\n",
    "# or 1GB of data whichever the condition meets first.\n",
    "# Note that storage_type can also be configured to point to cloud storage types on AWS, Azure, or GCP.\n",
    "# Ref:\n",
    "# https://developer.hashicorp.com/vault/tutorials/raft/raft-storage#automated-snapshots\n",
    "# https://developer.hashicorp.com/vault/api-docs/system/storage/raftautosnapshots\n",
    "vault write sys/storage/raft/snapshot-auto/config/daily interval=\"24h\" retain=7 \\\n",
    "  path_prefix=\"/vault\" storage_type=\"local\" local_max_space=1073741824\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View the automated snapshot configuration\n",
    "vault read sys/storage/raft/snapshot-auto/config/daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# You can also do a manual snapshot.  Note: Needs to be executed on the leader node.\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault login $VAULT_TOKEN\n",
    "echo\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault operator raft snapshot save /vault/demo.snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Let's do some modifications after the snapshot, let's enable the Transit engine.\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE --  vault secrets enable transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View that the transit engine is now in the list of secret engines.\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault secrets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Restore a snapshot.  Note: Needs to be executed on the leader node.\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault operator raft snapshot restore /vault/demo.snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Now view the list of secret engines and you will see that is has restored to the original state without the transit engine\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault secrets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster.  Verify that the cluster is still healthy after the restore.\n",
    "vault operator raft autopilot state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing High Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster and verify the leader is vault-0\n",
    "vault operator raft list-peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try deleting the vault-0 pod\n",
    "kubectl delete pod vault-0 -n $KUBENAMESPACE\n",
    "# See that the pod gets recreated\n",
    "kubectl get pods -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster and verify that another leader has taken over\n",
    "vault operator raft list-peers\n",
    "echo\n",
    "# Verify that vault-0 is sealed\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Verify that I can still access vault secrets even thought vault-0 is sealed\n",
    "vault secrets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster.  Verify that Healthy is false as the new Vault node is still sealed.\n",
    "vault operator raft autopilot state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Unseal vault-0 pod.  You should see Sealed = false.  Re-run the command if Sealed is true.\n",
    "echo \"Vault Unseal Key: $UNSEAL_KEY\"\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault operator unseal $UNSEAL_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster.  Verify that Healthy is now true.\n",
    "vault operator raft autopilot state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# The next demo will be showing Transit Auto Unseal.  Clearing the existing setup.\n",
    "# Delete the Vault cluster\n",
    "helm delete vault -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Clear Vault PVCs\n",
    "kubectl -n $KUBENAMESPACE delete pvc --all \n",
    "echo\n",
    "# Verify that all PVCs are cleared\n",
    "kubectl -n $KUBENAMESPACE get pvc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Vault Cluster for Transit Auto Unseal\n",
    "\n",
    "Auto-unseal can be done with AWS KMS, Azure Key Vault, GCP Cloud KMS, HSM devices via PKCS#11, and lastly Vault's Transit Engine.\n",
    "\n",
    "Ref: https://developer.hashicorp.com/vault/tutorials/auto-unseal\n",
    "\n",
    "This section will demonstrate how we can do auto-unsealing of the Vault cluster nodes using Transit Auto Unseal.\n",
    "\n",
    "Note that you can use the seal migration process.\n",
    "\n",
    "Ref: https://developer.hashicorp.com/vault/docs/concepts/seal#seal-migration\n",
    "\n",
    "For this demo purposes, we will be doing a fresh Vault cluster setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure a standalone Vault server to provide the auto unseal keys using Transit Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install a new Vault pod to provide the transit engine for auto-unseal\n",
    "helm install vault-transit hashicorp/vault --version 0.27.0 -n $KUBENAMESPACE -f - <<EOF\n",
    "injector:\n",
    "  enabled: false\n",
    "server:\n",
    "  image:\n",
    "    repository: hashicorp/vault-enterprise\n",
    "    tag: latest\n",
    "  enterpriseLicense:\n",
    "    secretName: vault-ent-license\n",
    "  logLevel: trace\n",
    "  auditStorage:\n",
    "    enabled: true\n",
    "  ha:\n",
    "    enabled: true\n",
    "    replicas: 1\n",
    "    raft:\n",
    "      enabled: true\n",
    "      setNodeId: true\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show resources in Vault namespace.  Verify that vault-transit-0 is Running before continuing.\n",
    "kubectl -n $KUBENAMESPACE get all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize vault-transit-0 pod.  For demo purposes, we will just be generating 1 unseal key.\n",
    "kubectl exec -ti vault-transit-0 -n $KUBENAMESPACE -- vault operator init -format=json -key-shares=1 -key-threshold=1 > transit-init.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show the transit-init.json\n",
    "cat transit-init.json | jq\n",
    "\n",
    "# Store the vault-transit-0 Unseal Key and Root Token for use later\n",
    "export TRANSIT_UNSEAL_KEY=$(jq -r '.unseal_keys_b64[]' transit-init.json)\n",
    "export TRANSIT_VAULT_TOKEN=$(jq -r '.root_token' transit-init.json)\n",
    "echo\n",
    "echo \"Transit Vault Unseal Key: $TRANSIT_UNSEAL_KEY\"\n",
    "echo \"Transit Vault Root Token: $TRANSIT_VAULT_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Unseal vault-transit-0 pod.  You should see Sealed = false\n",
    "echo \"Transit Vault Unseal Key: $TRANSIT_UNSEAL_KEY\"\n",
    "kubectl exec -ti vault-transit-0 -n $KUBENAMESPACE -- vault operator unseal $TRANSIT_UNSEAL_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configure the Transit Engine for use for Auto Unseal\n",
    "echo \"Transit Vault Root Token: $TRANSIT_VAULT_TOKEN\"\n",
    "# Login to Vault and enable the transit engine\n",
    "kubectl exec -ti vault-transit-0 -n $KUBENAMESPACE -- vault login $TRANSIT_VAULT_TOKEN\n",
    "echo\n",
    "kubectl exec -ti vault-transit-0 -n $KUBENAMESPACE -- vault secrets enable transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create an auto-unseal encryption key\n",
    "kubectl exec -ti vault-transit-0 -n $KUBENAMESPACE -- vault write -f transit/keys/autounseal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create an auto-unseal policy to access the autounseal key\n",
    "kubectl exec -ti vault-transit-0 -n $KUBENAMESPACE -- vault policy write autounseal - <<EOF\n",
    "path \"transit/encrypt/autounseal\" {\n",
    "    capabilities = [ \"update\" ]\n",
    "}\n",
    "path \"transit/decrypt/autounseal\" {\n",
    "    capabilities = [ \"update\" ]\n",
    "}\n",
    "EOF\n",
    "echo\n",
    "# Verify the policy is written\n",
    "kubectl exec -ti vault-transit-0 -n $KUBENAMESPACE -- vault policy read autounseal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create an orphaned token that will be auto-renewed by the transit auto-unseal (when it reaches 2/3 of expiry, it will try to renew)\n",
    "# Ref:\n",
    "# https://developer.hashicorp.com/vault/tutorials/auto-unseal/autounseal-transit\n",
    "# https://developer.hashicorp.com/vault/docs/commands/token/create#period\n",
    "kubectl exec -ti vault-transit-0 -n $KUBENAMESPACE -- vault token create -orphan -policy=\"autounseal\" \\\n",
    "    -period=24h -format=json > token.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show token.json\n",
    "cat token.json | jq\n",
    "\n",
    "# Store the Auto Unseal Token, this will be used to configure the Vault cluster for Auto Unseal\n",
    "export AUTOUNSEAL_TOKEN=$(jq -r .auth.client_token token.json)\n",
    "echo\n",
    "echo \"Auto Unseal Token: $AUTOUNSEAL_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Verify that the created token is valid and has the \"autounseal\" policy\n",
    "echo \"Transit Vault Auto Unseal Token: $AUTOUNSEAL_TOKEN\"\n",
    "kubectl exec -ti vault-transit-0 -n $KUBENAMESPACE -- vault login $AUTOUNSEAL_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# We will be showing the Vault upgrade process later\n",
    "# Ref: https://hub.docker.com/r/hashicorp/vault-enterprise/tags\n",
    "# Install latest minor release for Vault Enterprise 1.14 for now\n",
    "# Later we will be testing the upgrade to the latest minor release for Vault Enterprise 1.15\n",
    "export INITIAL_VAULT_VERSION=1.14-ent\n",
    "export TARGET_VAULT_VERSION=1.15-ent\n",
    "\n",
    "# Install the 3 node Vault cluster with the Vault helm chart.  Note the extra seal stanza.  \n",
    "# We will be setting the autounseal vault token as an environment variable.\n",
    "# See https://developer.hashicorp.com/vault/docs/platform/k8s/helm/configuration for options\n",
    "\n",
    "# The retry_join stanza is updated for 6 nodes to demo the autopilot upgrade use case below\n",
    "helm install vault hashicorp/vault --version 0.27.0 -n $KUBENAMESPACE -f - <<EOF\n",
    "injector:\n",
    "  enabled: false\n",
    "server:\n",
    "  image:\n",
    "    repository: hashicorp/vault-enterprise\n",
    "    tag: $INITIAL_VAULT_VERSION\n",
    "  enterpriseLicense:\n",
    "    secretName: vault-ent-license\n",
    "  logLevel: trace\n",
    "  auditStorage:\n",
    "    enabled: true\n",
    "  extraEnvironmentVars:\n",
    "    VAULT_TOKEN: $AUTOUNSEAL_TOKEN\n",
    "  ha:\n",
    "    enabled: true\n",
    "    replicas: 3\n",
    "    raft:\n",
    "      enabled: true\n",
    "      setNodeId: true\n",
    "      config: |\n",
    "        disable_mlock = true\n",
    "        ui = true\n",
    "        listener \"tcp\" {\n",
    "          tls_disable = 1\n",
    "          address = \"[::]:8200\"\n",
    "          cluster_address = \"[::]:8201\"\n",
    "        }\n",
    "        storage \"raft\" {\n",
    "          # PVC Volume to keep Vault data\n",
    "          path = \"/vault/data\"\n",
    "          # For auto-join to the raft cluster\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-0.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-1.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-2.vault-internal:8200\"\n",
    "          } \n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-3.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-4.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-5.vault-internal:8200\"\n",
    "          } \n",
    "        }\n",
    "        seal \"transit\" {\n",
    "          address = \"http://vault-transit-0.vault-transit-internal:8200\"\n",
    "          #token = $AUTOUNSEAL_TOKEN\n",
    "          disable_renewal = \"false\"\n",
    "          key_name = \"autounseal\"\n",
    "          mount_path = \"transit/\"\n",
    "          tls_skip_verify = \"true\"\n",
    "        } \n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show resources in Vault namespace\n",
    "kubectl -n $KUBENAMESPACE get all\n",
    "\n",
    "# Make sure all Vault pods are in Running status before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize vault-0 pod.  Note that the flags we use are for recovery keys as this is a auto unseal setup.\n",
    "# For demo purposes, we will just be generating 1 recovery key.\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault operator init -format=json -recovery-shares=1 -recovery-threshold=1 > init.json\n",
    "cat init.json | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Verify that all Vault nodes are already unsealed before going to the next step.  Sealed = false\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault status\n",
    "echo\n",
    "kubectl exec -ti vault-1 -n $KUBENAMESPACE -- vault status\n",
    "echo\n",
    "kubectl exec -ti vault-2 -n $KUBENAMESPACE -- vault status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster.  Verify that Healthy is now true.\n",
    "export VAULT_TOKEN=$(jq -r '.root_token' init.json)\n",
    "echo \"Vault Root Token: $VAULT_TOKEN\"\n",
    "echo\n",
    "vault operator raft autopilot state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try deleting the vault-0 pod\n",
    "kubectl delete pod vault-0 -n $KUBENAMESPACE\n",
    "# See that the pod gets recreated\n",
    "kubectl get pods -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Verify that the vault-0 pod got recreated\n",
    "kubectl get pods -o=wide -n $KUBENAMESPACE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Verify that recreated Vault node is already unsealed.  Sealed = false\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- vault status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrading a Vault Cluster on K8s with Integrated Storage Autopilot\n",
    "\n",
    "This will demostrate the upgrade process for a Vault Cluster on K8s and how integration storage autopilot does the automatic promotion of the leader/follower nodes to the new K8s pods with the newer Vault version.\n",
    "- Update helm chart and scale out the Vault cluster to 6 nodes with the target Vault version.  i.e. 3 current nodes + 3 newer nodes\n",
    "- Let autopilot handle the automatic promotion of the new nodes to be the leader/followers.\n",
    "- Once, the autopilot process is complete, delete the old 3 Vault nodes.  3 new Vault nodes will be recreated on the target Vault version to form a 6 node cluster.\n",
    "- Remove Vault nodes 4-6 from the raft setup.  i.e. Demote back to a 3 Vault node cluster.\n",
    "- Scale the cluster back down from 6 nodes to 3 nodes to remove Vault nodes 4-6.\n",
    "\n",
    "Ref:\n",
    "https://developer.hashicorp.com/vault/tutorials/kubernetes/kubernetes-raft-deployment-guide#upgrading-vault-on-kubernetes\n",
    "https://developer.hashicorp.com/vault/tutorials/raft/raft-autopilot\n",
    "\n",
    "The Vault StatefulSet uses OnDelete update strategy. It is critical to use OnDelete instead of RollingUpdate because standbys must be updated before the active primary. A failover to an older version of Vault must always be avoided.\n",
    "\n",
    "Important: For a Kubernetes StatefulSet with N replicas, note that pods are deleted in the reverse order that they are created. i.e. From N-1 to 0.\n",
    "\n",
    "Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees\n",
    "\n",
    "Note: We will be using the previous Vault 3-node cluster with transit auto unseal that was configured earlier. So the previous setup needs to be executed before running this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster\n",
    "vault operator raft list-peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# We will be using the same yaml, the only change is the server.image.tag and server.ha.replicas values.\n",
    "# We will be scaling to 6 nodes.  3 nodes (current Vault version) + 3 nodes (new Vault version).\n",
    "helm upgrade vault hashicorp/vault --version 0.27.0 -n $KUBENAMESPACE -f - <<EOF\n",
    "injector:\n",
    "  enabled: false\n",
    "server:\n",
    "  image:\n",
    "    repository: hashicorp/vault-enterprise\n",
    "    tag: $TARGET_VAULT_VERSION\n",
    "  enterpriseLicense:\n",
    "    secretName: vault-ent-license\n",
    "  logLevel: trace\n",
    "  auditStorage:\n",
    "    enabled: true\n",
    "  extraEnvironmentVars:\n",
    "    VAULT_TOKEN: $AUTOUNSEAL_TOKEN\n",
    "  ha:\n",
    "    enabled: true\n",
    "    replicas: 6\n",
    "    raft:\n",
    "      enabled: true\n",
    "      setNodeId: true\n",
    "      config: |\n",
    "        disable_mlock = true\n",
    "        ui = true\n",
    "        listener \"tcp\" {\n",
    "          tls_disable = 1\n",
    "          address = \"[::]:8200\"\n",
    "          cluster_address = \"[::]:8201\"\n",
    "        }\n",
    "        storage \"raft\" {\n",
    "          # PVC Volume to keep Vault data\n",
    "          path = \"/vault/data\"\n",
    "          # For auto-join to the raft cluster\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-0.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-1.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-2.vault-internal:8200\"\n",
    "          } \n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-3.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-4.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-5.vault-internal:8200\"\n",
    "          } \n",
    "        }\n",
    "        seal \"transit\" {\n",
    "          address = \"http://vault-transit-0.vault-transit-internal:8200\"\n",
    "          #token = $AUTOUNSEAL_TOKEN\n",
    "          disable_renewal = \"false\"\n",
    "          key_name = \"autounseal\"\n",
    "          mount_path = \"transit/\"\n",
    "          tls_skip_verify = \"true\"\n",
    "        } \n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster and see that autopilot is slowing promoting the new nodes and demoting the old nodes\n",
    "# Re-run this command to see the following changes happening\n",
    "# - new nodes getting added as non-voters\n",
    "# - autopilot kicks in and new nodes get promoted to voters\n",
    "# - leader is transferred to the new nodes\n",
    "# - the old nodes are demoted to non-voters\n",
    "vault operator raft list-peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster.  Verify that Healthy is true and the voter nodes are the new Vault nodes.\n",
    "# The older version Vault nodes are now non-voters.\n",
    "vault operator raft autopilot state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Now delete vault-0, vault-1, vault-2 pods and get them to upgrade to the target version\n",
    "kubectl delete pod vault-0 vault-1 vault-2 -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# See that the vault-0, vault-1, and vault-2 pods gets recreated.  When all the pods are in running state, go to the next step\n",
    "kubectl get pods -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster and verify that the recreated Vault nodes are coming back as voter nodes\n",
    "vault operator raft list-peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster.  Verify that all nodes on are on target version now.\n",
    "# Note down which is the current leader node.\n",
    "vault operator raft autopilot state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# For a Kubernetes StatefulSet with N replicas, note that pods are deleted in the reverse order that they are created.\n",
    "# i.e. From N-1 to 0.\n",
    "# Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees\n",
    "# Now we will be scaling back to our original 3 nodes.  We will be removing vault-3, vault-4, vault-5 from the raft cluster.\n",
    "\n",
    "# Tip:\n",
    "# The removal of the leader node takes a bit of time for the re-election to occur.\n",
    "# So for vault-3, vault-4, vault-5, one of them is the current leader node.\n",
    "# We do not want any of these removed nodes to be re-elected.\n",
    "# So run the following removals in the correct sequence accordingly.\n",
    "# i.e. Remove the follower nodes first before removing the leader node.  \n",
    "\n",
    "# Remove vault-3 from raft cluster\n",
    "vault operator raft remove-peer vault-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Remove vault-4 from raft cluster\n",
    "vault operator raft remove-peer vault-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Remove vault-5 from raft cluster\n",
    "vault operator raft remove-peer vault-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View raft status of your Vault cluster and verify that it now only has the original first three nodes.\n",
    "# i.e. vault-0, vault-1, vault-2\n",
    "vault operator raft list-peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Now scale back our helm chart to 3 replicas\n",
    "# server.image.tag is left at the target version and server.ha.replicas is changed from 6 to 3.\n",
    "helm upgrade vault hashicorp/vault --version 0.27.0 -n $KUBENAMESPACE -f - <<EOF\n",
    "injector:\n",
    "  enabled: false\n",
    "server:\n",
    "  image:\n",
    "    repository: hashicorp/vault-enterprise\n",
    "    tag: $TARGET_VAULT_VERSION\n",
    "  enterpriseLicense:\n",
    "    secretName: vault-ent-license\n",
    "  logLevel: trace\n",
    "  auditStorage:\n",
    "    enabled: true\n",
    "  extraEnvironmentVars:\n",
    "    VAULT_TOKEN: $AUTOUNSEAL_TOKEN\n",
    "  ha:\n",
    "    enabled: true\n",
    "    replicas: 3\n",
    "    raft:\n",
    "      enabled: true\n",
    "      setNodeId: true\n",
    "      config: |\n",
    "        disable_mlock = true\n",
    "        ui = true\n",
    "        listener \"tcp\" {\n",
    "          tls_disable = 1\n",
    "          address = \"[::]:8200\"\n",
    "          cluster_address = \"[::]:8201\"\n",
    "        }\n",
    "        storage \"raft\" {\n",
    "          # PVC Volume to keep Vault data\n",
    "          path = \"/vault/data\"\n",
    "          # For auto-join to the raft cluster\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-0.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-1.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-2.vault-internal:8200\"\n",
    "          } \n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-3.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-4.vault-internal:8200\"\n",
    "          }\n",
    "          retry_join {\n",
    "            leader_api_addr = \"http://vault-5.vault-internal:8200\"\n",
    "          } \n",
    "        }\n",
    "        seal \"transit\" {\n",
    "          address = \"http://vault-transit-0.vault-transit-internal:8200\"\n",
    "          #token = $AUTOUNSEAL_TOKEN\n",
    "          disable_renewal = \"false\"\n",
    "          key_name = \"autounseal\"\n",
    "          mount_path = \"transit/\"\n",
    "          tls_skip_verify = \"true\"\n",
    "        } \n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# You will now see that we have gone back to the original 3 Vault nodes configuration\n",
    "kubectl get pods -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Note that Vault nodes 4-6 PVCs are still there\n",
    "kubectl -n $KUBENAMESPACE get pvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Delete the PVCs used by Vault nodes 4-6\n",
    "kubectl -n $KUBENAMESPACE delete pvc audit-vault-3 audit-vault-4 audit-vault-5 data-vault-3 data-vault-4 data-vault-5\n",
    "\n",
    "# Verify that Vault nodes 4-6 PVCs are removed\n",
    "kubectl -n $KUBENAMESPACE get pvc\n",
    "\n",
    "# This completes the autopilot upgrade process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up temp files\n",
    "rm init.json\n",
    "rm transit-init.json\n",
    "rm token.json\n",
    "\n",
    "# Uninstall metrics server\n",
    "helm delete metrics-server -n kube-system\n",
    "\n",
    "# Disable file audit device\n",
    "vault audit disable file\n",
    "\n",
    "# Remove the NodePort\n",
    "kubectl delete svc port-vault-svc -n $KUBENAMESPACE \n",
    "\n",
    "# Delete Vault cluster\n",
    "helm delete vault -n $KUBENAMESPACE\n",
    "\n",
    "# Delete the Vault for Transit Auto Unseal\n",
    "helm delete vault-transit -n $KUBENAMESPACE\n",
    "\n",
    "# Clear Vault PVCs\n",
    "kubectl -n $KUBENAMESPACE delete pvc --all \n",
    "\n",
    "# Delete kind cluster\n",
    "kind delete cluster --name vault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - Other Useful Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Optional: Turn on the file audit device, this allows you to keep a detailed log of all requests to Vault\n",
    "vault audit enable file file_path=/vault/audit/vault_audit.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Optional: view pod logs\n",
    "kubectl logs vault-0 -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Optional: view pod details\n",
    "kubectl describe pod vault-0 -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Optional: Add metrics-server to be able to view CPU and memory usage\n",
    "helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/\n",
    "helm repo update\n",
    "helm upgrade --install --set args={--kubelet-insecure-tls} metrics-server metrics-server/metrics-server --namespace kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Optional: You can use k9s to view your pods.\n",
    "# You can also use the following commands to see the utlization on your nodes/pods\n",
    "kubectl top nodes\n",
    "echo\n",
    "kubectl top pod -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# To get a shell into a Vault pod\n",
    "kubectl exec -ti vault-0 -n $KUBENAMESPACE -- /bin/sh\n",
    "#kubectl exec -ti vault-0 -n vault-ns -- /bin/sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show ConfigMap resources for Vault\n",
    "kubectl get configmap -n $KUBENAMESPACE -o=yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show vault-config details\n",
    "kubectl describe configmaps vault-config -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show Vault pod details\n",
    "kubectl describe pod vault-0 -n $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show Persistent Volume Claims in use by Vault\n",
    "kubectl get pvc -n $KUBENAMESPACE -o=yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
